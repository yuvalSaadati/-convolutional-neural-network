{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def load_data(filename):\n",
    "    \"\"\"\n",
    "    Loads data from a CSV file and processes it into suitable format.\n",
    "    For training and validation data, it splits into features and labels.\n",
    "    For test data, it returns only features and assumes the first column is a placeholder.\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(filename, header=None)\n",
    "    if 'train' in filename or 'validate' in filename:\n",
    "        # Split into features and labels\n",
    "        labels = data.iloc[:, 0].values - 1  # Convert class labels to zero-indexed\n",
    "        features = data.iloc[:, 1:].values\n",
    "    else:\n",
    "        # Test data does not include labels\n",
    "        labels = None\n",
    "        features = data.iloc[:, 1:].values  # Ignore the placeholder column\n",
    "    \n",
    "    # Reshape features into 32x32x3 format for RGB images\n",
    "    features = features.reshape((-1, 32, 32, 3))\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "# Example usage\n",
    "train_features, train_labels = load_data('./data/train.csv')\n",
    "validate_features, validate_labels = load_data('./data/validate.csv')\n",
    "test_features, _ = load_data('./data/test.csv')  # Test data doesn't have labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Define Helper Functions for Each Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "def dropout_forward(A, dropout_rate):\n",
    "    \"\"\"\n",
    "    Applies dropout to the activations A.\n",
    "    \n",
    "    Parameters:\n",
    "      A (np.ndarray): Activations from a layer (any shape).\n",
    "      dropout_rate (float): Probability of dropping a unit (e.g., 0.5 for 50% dropout).\n",
    "      \n",
    "    Returns:\n",
    "      A_dropout (np.ndarray): The activations after dropout is applied.\n",
    "      mask (np.ndarray): The dropout mask used (should be stored for the backward pass).\n",
    "    \"\"\"\n",
    "    # Create a mask that drops neurons with probability dropout_rate.\n",
    "    # We divide by (1 - dropout_rate) so that the expected value of the activations remains unchanged.\n",
    "    mask = (np.random.rand(*A.shape) >= dropout_rate) / (1 - dropout_rate)\n",
    "    A_dropout = A * mask\n",
    "    return A_dropout, mask\n",
    "\n",
    "def convolve2d(images, kernel, bias=None, stride=1, padding=1):\n",
    "    \"\"\"\n",
    "    Convolve a batch of images with a set of kernels using a vectorized approach.\n",
    "    \n",
    "    Parameters:\n",
    "      images : numpy.ndarray\n",
    "          Input tensor of shape (batch_size, image_height, image_width, image_channels).\n",
    "      kernel : numpy.ndarray\n",
    "          Convolution kernel tensor of shape (kernel_height, kernel_width, image_channels, num_kernels).\n",
    "      bias : numpy.ndarray, optional\n",
    "          Bias tensor of shape (1, 1, 1, num_kernels). If provided, will be added to the output.\n",
    "      stride : int, optional\n",
    "          Stride length. (Default is 1)\n",
    "      padding : int, optional\n",
    "          Padding width applied to the input images. (Default is 1)\n",
    "    \n",
    "    Returns:\n",
    "      output : numpy.ndarray\n",
    "          Convolved output of shape (batch_size, out_height, out_width, num_kernels).\n",
    "      cache : dict\n",
    "          Dictionary with values needed for the backward pass.\n",
    "    \"\"\"\n",
    "    # Unpack dimensions.\n",
    "    batch_size, image_height, image_width, image_channels = images.shape\n",
    "    kernel_height, kernel_width, kernel_channels, num_kernels = kernel.shape\n",
    "\n",
    "    # Ensure the kernel depth matches the image depth.\n",
    "    assert kernel_channels == image_channels, \"Kernel and image channels must match.\"\n",
    "\n",
    "    # Apply padding.\n",
    "    padded_images = np.pad(images, \n",
    "                           ((0, 0), (padding, padding), (padding, padding), (0, 0)), \n",
    "                           mode='constant')\n",
    "    padded_height = image_height + 2 * padding\n",
    "    padded_width = image_width + 2 * padding\n",
    "\n",
    "    # Calculate output dimensions.\n",
    "    out_height = (padded_height - kernel_height) // stride + 1\n",
    "    out_width = (padded_width - kernel_width) // stride + 1\n",
    "\n",
    "    # Create a sliding window view of the padded images.\n",
    "    # The new shape is (batch_size, out_height, out_width, kernel_height, kernel_width, image_channels)\n",
    "    shape = (batch_size, out_height, out_width, kernel_height, kernel_width, image_channels)\n",
    "    strides = (padded_images.strides[0],\n",
    "               stride * padded_images.strides[1],\n",
    "               stride * padded_images.strides[2],\n",
    "               padded_images.strides[1],\n",
    "               padded_images.strides[2],\n",
    "               padded_images.strides[3])\n",
    "    windows = as_strided(padded_images, shape=shape, strides=strides)\n",
    "\n",
    "    # Perform the convolution using tensordot.\n",
    "    # This contracts over the window dimensions (kernel_height, kernel_width, image_channels),\n",
    "    # resulting in an output of shape (batch_size, out_height, out_width, num_kernels).\n",
    "    output = np.tensordot(windows, kernel, axes=([3, 4, 5], [0, 1, 2]))\n",
    "\n",
    "    # Add bias if provided.\n",
    "    if bias is not None:\n",
    "        # Ensure the bias has the shape (1, 1, 1, num_kernels) for broadcasting.\n",
    "        output = output + bias\n",
    "\n",
    "    # Store necessary values for backpropagation.\n",
    "    cache = {\n",
    "        \"images_padded\": padded_images,\n",
    "        \"kernel\": kernel,\n",
    "        \"bias\": bias,\n",
    "        \"padding\": padding,\n",
    "        \"stride\": stride\n",
    "    }\n",
    "\n",
    "    return output, cache\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return np.where(x > 0, x, 0.01 * x)\n",
    "    #return np.maximum(0, x)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "\n",
    "def max_pooling(images, size=2, stride=2):\n",
    "    \"\"\"\n",
    "    Perform max pooling on a 4D tensor of images with shape\n",
    "    (batch_size, image_height, image_width, num_feature_maps).\n",
    "\n",
    "    Returns:\n",
    "      - output: pooled output with shape (batch_size, out_height, out_width, num_feature_maps)\n",
    "      - cache: tuple containing (images, mask, size, stride) for backpropagation.\n",
    "    \"\"\"\n",
    "    batch_size, image_height, image_width, num_feature_maps = images.shape\n",
    "    out_height = (image_height - size) // stride + 1\n",
    "    out_width = (image_width - size) // stride + 1\n",
    "\n",
    "    # Create a sliding window view of the images:\n",
    "    # The new shape will be (batch_size, out_height, out_width, size, size, num_feature_maps)\n",
    "    new_shape = (batch_size, out_height, out_width, size, size, num_feature_maps)\n",
    "    new_strides = (images.strides[0],\n",
    "                   stride * images.strides[1],\n",
    "                   stride * images.strides[2],\n",
    "                   images.strides[1],\n",
    "                   images.strides[2],\n",
    "                   images.strides[3])\n",
    "    windows = as_strided(images, shape=new_shape, strides=new_strides)\n",
    "\n",
    "    # Compute the max over each window (axes 3 and 4 correspond to the pooling window)\n",
    "    output = np.max(windows, axis=(3, 4))\n",
    "\n",
    "    # To build the mask, first reshape the windows so each window becomes a vector.\n",
    "    windows_reshaped = windows.reshape(batch_size, out_height, out_width, size * size, num_feature_maps)\n",
    "    # Find the indices of the maximum values within each window.\n",
    "    argmax_indices = np.argmax(windows_reshaped, axis=3)  # shape: (batch_size, out_height, out_width, num_feature_maps)\n",
    "    # Convert the flattened indices back into 2D indices (row, col) inside the pooling window.\n",
    "    max_row = argmax_indices // size  # integer division gives the row\n",
    "    max_col = argmax_indices % size   # modulo gives the col\n",
    "\n",
    "    # Build the mask array of zeros (same shape as images)\n",
    "    mask = np.zeros_like(images)\n",
    "\n",
    "    # Create index arrays for each dimension\n",
    "    # batch indices with shape (batch_size, 1, 1, 1)\n",
    "    batch_range = np.arange(batch_size)[:, None, None, None]\n",
    "    # y and x indices for the pooled output with shapes (1, out_height, 1, 1) and (1, 1, out_width, 1)\n",
    "    y_range = np.arange(out_height)[None, :, None, None]\n",
    "    x_range = np.arange(out_width)[None, None, :, None]\n",
    "    # Channel indices with shape (1, 1, 1, num_feature_maps)\n",
    "    channel_range = np.arange(num_feature_maps)[None, None, None, :]\n",
    "\n",
    "    # Compute the actual row and column indices in the original images:\n",
    "    # For each pooling window, the top-left corner in the original image is:\n",
    "    # (y_range*stride, x_range*stride). Then add the offset from max pooling (max_row, max_col).\n",
    "    row_indices = y_range * stride + max_row  # shape: (batch_size, out_height, out_width, num_feature_maps)\n",
    "    col_indices = x_range * stride + max_col  # shape: (batch_size, out_height, out_width, num_feature_maps)\n",
    "\n",
    "    # Use advanced indexing to set the mask to 1 at the maximum locations.\n",
    "    mask[batch_range, row_indices, col_indices, channel_range] = 1\n",
    "\n",
    "    cache = (images, mask, size, stride)\n",
    "    return output, cache\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fully_connected(x, weights, biases):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer and return both the output and a cache for the backward pass.\n",
    "    \n",
    "    Parameters:\n",
    "    x (numpy.ndarray): Input data or features.\n",
    "    weights (numpy.ndarray): Weights matrix of the fully connected layer.\n",
    "    biases (numpy.ndarray): Biases vector of the fully connected layer.\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: Output of the fully connected layer.\n",
    "    tuple: Cache containing inputs and weights needed for the backward pass.\n",
    "    \"\"\"\n",
    "    # Compute the output of the fully connected layer\n",
    "    y = np.dot(x, weights) + biases\n",
    "    cache = {\"input\": x, \"weights\": weights, \"biases\": biases}\n",
    "    return y, cache\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Forward and Backward Propagation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(logits):\n",
    "    \"\"\"\n",
    "    Apply the softmax function to the last layer's output to convert logits into probabilities.\n",
    "\n",
    "    Args:\n",
    "    logits (numpy.ndarray): Logits array from the last fully connected layer of shape (batch_size, num_classes).\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The probabilities after applying softmax of shape (batch_size, num_classes).\n",
    "    \"\"\"\n",
    "    # Shift the logits by subtracting the maximum value to prevent large exponentials\n",
    "    exp_shifted = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
    "    # Normalize the exponentials so that the sum of each row is 1, resulting in probabilities\n",
    "    probabilities = exp_shifted / np.sum(exp_shifted, axis=1, keepdims=True)\n",
    "    \n",
    "    return probabilities\n",
    "def batch_norm(x, epsilon=1e-5):\n",
    "    mean = np.mean(x, axis=0, keepdims=True)\n",
    "    variance = np.var(x, axis=0, keepdims=True)\n",
    "    return (x - mean) / np.sqrt(variance + epsilon)\n",
    "\n",
    "def forward_pass(X_batch, params, dropout_rate=0.5, training=True):\n",
    "    \"\"\"\n",
    "    Forward pass through the CNN with two convolutional layers and two fully connected layers,\n",
    "    with batch normalization and dropout applied.\n",
    "    \n",
    "    Parameters:\n",
    "      X_batch (np.ndarray): Input batch.\n",
    "      params (dict): Model parameters.\n",
    "      dropout_rate (float): Dropout probability.\n",
    "      training (bool): Flag indicating whether to apply dropout (True during training).\n",
    "      \n",
    "    Returns:\n",
    "      softmax_output (np.ndarray): Final output probabilities.\n",
    "      caches (dict): A dictionary of caches needed for the backward pass.\n",
    "    \"\"\"\n",
    "    caches = {}\n",
    "\n",
    "    # First Convolutional Layer\n",
    "    conv1_output, conv1_cache = convolve2d(X_batch, params['conv1_w'], bias=params['conv1_b'])\n",
    "    bn1_output = batch_norm(conv1_output) if training else conv1_output  # Apply Batch Normalization only during training\n",
    "    relu1_output = relu(bn1_output)        # Apply ReLU\n",
    "    relu1_cache = bn1_output\n",
    "    pool1_output, pool1_cache = max_pooling(relu1_output)\n",
    "\n",
    "    # Second Convolutional Layer\n",
    "    conv2_output, conv2_cache = convolve2d(pool1_output, params['conv2_w'], bias=params['conv2_b'])\n",
    "    bn2_output = batch_norm(conv2_output) if training else conv2_output  # Apply Batch Normalization only during training\n",
    "    relu2_output = relu(bn2_output)        # Apply ReLU\n",
    "    relu2_cache = bn2_output\n",
    "    pool2_output, pool2_cache = max_pooling(relu2_output)\n",
    "\n",
    "    # Flatten for fully connected layers\n",
    "    batch_size = pool2_output.shape[0]\n",
    "    flattened_pool_output = pool2_output.reshape(batch_size, -1)\n",
    "\n",
    "    # First Fully Connected Layer\n",
    "    fc1_output, fc1_cache = fully_connected(flattened_pool_output, params['fc1_w'], params['fc1_b'])\n",
    "    bn_fc1_output = batch_norm(fc1_output) if training else fc1_output  # Apply Batch Normalization only during training\n",
    "    fc1_output = relu(bn_fc1_output)       # Apply ReLU\n",
    "    fc1_cache['output_shape'] = pool2_output.shape\n",
    "\n",
    "    # Apply dropout after the first fully connected layer (only during training)\n",
    "    if training:\n",
    "        fc1_output, dropout1_cache = dropout_forward(fc1_output, dropout_rate)\n",
    "        fc1_cache['dropout_cache'] = dropout1_cache\n",
    "    else:\n",
    "        fc1_cache['dropout_cache'] = None\n",
    "\n",
    "    # Second Fully Connected Layer (Output Layer)\n",
    "    fc2_output, fc2_cache = fully_connected(fc1_output, params['fc2_w'], params['fc2_b'])\n",
    "    fc2_cache['output_shape'] = fc2_output.shape\n",
    "\n",
    "    # Apply softmax activation to get final probabilities\n",
    "    softmax_output = softmax(fc2_output)\n",
    "\n",
    "    # Store caches for backward pass\n",
    "    caches = {\n",
    "        'conv1_cache': conv1_cache,\n",
    "        'relu1_cache': relu1_cache,\n",
    "        'pool1_cache': pool1_cache,\n",
    "        'conv2_cache': conv2_cache,\n",
    "        'relu2_cache': relu2_cache,\n",
    "        'pool2_cache': pool2_cache,\n",
    "        'fc1_cache': fc1_cache,\n",
    "        'fc2_cache': fc2_cache\n",
    "    }\n",
    "    return softmax_output, caches\n",
    "\n",
    "\n",
    "\n",
    "def compute_loss(probs, labels):\n",
    "    batch_size = probs.shape[0]\n",
    "    num_classes = probs.shape[1]\n",
    "    label_smoothing = 0.1\n",
    "    true_labels = np.eye(num_classes)[labels] * (1 - label_smoothing) + (label_smoothing / num_classes)\n",
    "\n",
    "    # true_labels = np.eye(num_classes)[labels] \n",
    "    cross_entropy_loss = -np.sum(true_labels * np.log(probs + 1e-12)) / batch_size\n",
    "    return cross_entropy_loss\n",
    "\n",
    "# def compute_loss(predictions, targets):\n",
    "\n",
    "#     num_samples = 10\n",
    "#     num_classes = predictions.shape[1]\n",
    "#     # Avoid numerical instability by adding a small epsilon value\n",
    "#     epsilon = 1e-7\n",
    "#     predictions = np.clip(predictions, epsilon, 1 - epsilon)\n",
    "#     true_labels = np.eye(num_classes)[targets] \n",
    "#     loss = -np.sum(true_labels * np.log(predictions)) / num_samples\n",
    "#     return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training Loop and Parameter Updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "\n",
    "def dropout_backward(dA, mask):\n",
    "    \"\"\"\n",
    "    Backward pass for dropout.\n",
    "    \n",
    "    Parameters:\n",
    "      dA (np.ndarray): Gradient of the loss with respect to the dropout layer's output.\n",
    "      mask (np.ndarray): Dropout mask that was used in the forward pass.\n",
    "      \n",
    "    Returns:\n",
    "      dA_prev (np.ndarray): Gradient of the loss with respect to the dropout layer's input.\n",
    "    \"\"\"\n",
    "    dA_prev = dA * mask\n",
    "    return dA_prev\n",
    "\n",
    "def conv_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Optimized backward pass for a convolutional layer.\n",
    "    \n",
    "    Args:\n",
    "      dout (numpy.ndarray): Upstream gradients of shape \n",
    "                            (batch_size, output_height, output_width, num_filters).\n",
    "      cache (dict): Dictionary containing:\n",
    "          - 'images_padded': Padded input images of shape (batch_size, padded_height, padded_width, num_channels).\n",
    "          - 'kernel': Convolution kernel of shape (kernel_height, kernel_width, num_channels, num_filters).\n",
    "          - 'stride': Stride used during the forward pass.\n",
    "          - 'padding': Padding applied during the forward pass.\n",
    "    \n",
    "    Returns:\n",
    "      dA_prev (numpy.ndarray): Gradient with respect to the input images (without padding).\n",
    "      dW (numpy.ndarray): Gradient with respect to the kernel weights.\n",
    "      db (numpy.ndarray): Gradient with respect to the biases.\n",
    "    \"\"\"\n",
    "    # Extract information from the cache.\n",
    "    images_padded = cache['images_padded']  # Padded input images.\n",
    "    kernel = cache['kernel']\n",
    "    stride = cache['stride']\n",
    "    padding = cache['padding']\n",
    "\n",
    "    # Unpack shapes.\n",
    "    batch_size, padded_height, padded_width, num_channels = images_padded.shape\n",
    "    kernel_height, kernel_width, _, num_filters = kernel.shape\n",
    "    _, out_height, out_width, _ = dout.shape\n",
    "\n",
    "    # -----------------------\n",
    "    # Compute dW (gradient with respect to the kernel weights)\n",
    "    # -----------------------\n",
    "    # Create a sliding window view of the padded images.\n",
    "    shape = (batch_size, out_height, out_width, kernel_height, kernel_width, num_channels)\n",
    "    strides = (images_padded.strides[0],\n",
    "               stride * images_padded.strides[1],\n",
    "               stride * images_padded.strides[2],\n",
    "               images_padded.strides[1],\n",
    "               images_padded.strides[2],\n",
    "               images_padded.strides[3])\n",
    "    windows = as_strided(images_padded, shape=shape, strides=strides)\n",
    "    \n",
    "    # Use tensordot to contract over batch, out_height, and out_width.\n",
    "    # This produces dW with shape (kernel_height, kernel_width, num_channels, num_filters).\n",
    "    dW = np.tensordot(windows, dout, axes=([0, 1, 2], [0, 1, 2]))\n",
    "    \n",
    "    # -----------------------\n",
    "    # Compute db (gradient with respect to the biases)\n",
    "    # -----------------------\n",
    "    # Sum over batch, out_height, and out_width.\n",
    "    db = np.sum(dout, axis=(0, 1, 2), keepdims=True)  # Shape: (1, 1, 1, num_filters)\n",
    "    \n",
    "    # -----------------------\n",
    "    # Compute dA_prev (gradient with respect to the input images)\n",
    "    # -----------------------\n",
    "    # Initialize gradient for padded input.\n",
    "    dA_prev_padded = np.zeros_like(images_padded)\n",
    "    \n",
    "    # Instead of four nested loops, we loop over the kernel dimensions only.\n",
    "    # For each (i, j) position in the kernel, add contributions to the appropriate\n",
    "    # positions in dA_prev_padded.\n",
    "    for i in range(kernel_height):\n",
    "        for j in range(kernel_width):\n",
    "            # The slice of dA_prev_padded to update:\n",
    "            # It starts at index i and j, and then covers the region that the kernel \"saw\"\n",
    "            # during the forward pass. The region has shape (batch_size, out_height, out_width, num_channels).\n",
    "            i_end = i + out_height * stride\n",
    "            j_end = j + out_width * stride\n",
    "            \n",
    "            # Using np.einsum to multiply dout (shape: [batch_size, out_height, out_width, num_filters])\n",
    "            # with the kernel slice (shape: [num_channels, num_filters]).\n",
    "            # The einsum 'bxyf,cf->bxyc' computes, for each batch b and spatial position (x,y),\n",
    "            # the sum over f: dout[b,x,y,f] * kernel[i, j, c, f], resulting in shape (batch_size, out_height, out_width, num_channels).\n",
    "            dA_prev_padded[:, i:i_end:stride, j:j_end:stride, :] += np.einsum('bxyf,cf->bxyc', dout, kernel[i, j, :, :])\n",
    "\n",
    "    # Remove the padding to obtain the gradient with respect to the original images.\n",
    "    if padding != 0:\n",
    "        dA_prev = dA_prev_padded[:, padding:-padding, padding:-padding, :]\n",
    "    else:\n",
    "        dA_prev = dA_prev_padded\n",
    "\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "  \n",
    "def relu_backward(dout, relu_cache):\n",
    "    \"\"\"\n",
    "    Backward pass for the ReLU activation function.\n",
    "\n",
    "    Parameters:\n",
    "      dout: Upstream derivatives.\n",
    "      relu_cache: Cache from the forward pass, typically the input to the ReLU.\n",
    "\n",
    "    Returns:\n",
    "      The gradient with respect to the input of the ReLU.\n",
    "    \"\"\"\n",
    "    # Only propagate the gradient for positive inputs.\n",
    "    return dout * (relu_cache > 0)\n",
    "\n",
    "\n",
    "def pool_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Backward pass for the max pooling layer.\n",
    "    \n",
    "    Parameters:\n",
    "      dout: Gradient of the loss with respect to the output of the pooling layer.\n",
    "            Shape: (batch_size, out_height, out_width, channels)\n",
    "      cache: Tuple containing (image, mask, size, stride), where:\n",
    "             - image: the original input to the pooling layer, shape (batch_size, image_height, image_width, channels)\n",
    "             - mask: a binary mask of the same shape as image indicating the positions of the max values during the forward pass.\n",
    "             - size: the pooling window size.\n",
    "             - stride: the stride used during pooling.\n",
    "             \n",
    "    Returns:\n",
    "      dimage: Gradient of the loss with respect to the input of the pooling layer.\n",
    "    \"\"\"\n",
    "    image, mask, size, stride = cache\n",
    "    dimage = np.zeros_like(image)\n",
    "    \n",
    "    batch_size, image_height, image_width, image_channels = image.shape\n",
    "    _, out_height, out_width, _ = dout.shape\n",
    "    \n",
    "    # Loop over the batch, spatial dimensions, and channels.\n",
    "    for i in range(batch_size):\n",
    "        for z in range(image_channels):\n",
    "            for y in range(out_height):\n",
    "                for x in range(out_width):\n",
    "                    # Calculate the window boundaries.\n",
    "                    start_y = y * stride\n",
    "                    start_x = x * stride\n",
    "                    end_y = start_y + size\n",
    "                    end_x = start_x + size\n",
    "\n",
    "                    # Extract the mask window corresponding to this pooling region.\n",
    "                    window = mask[i, start_y:end_y, start_x:end_x, z]\n",
    "                    \n",
    "                    # Propagate the gradient only to the max element(s) (where mask == 1).\n",
    "                    dimage[i, start_y:end_y, start_x:end_x, z] += window * dout[i, y, x, z]\n",
    "    \n",
    "    return dimage\n",
    "\n",
    "\n",
    "def fc_backward(dout, weights, input_cache):\n",
    "    \"\"\"\n",
    "    Backward pass for the fully connected layer.\n",
    "    \n",
    "    Parameters:\n",
    "      dout: Upstream derivatives, expected shape (batch_size, num_classes)\n",
    "            or (num_classes,) if a single example is provided.\n",
    "      weights: Weights used in the forward pass, shape (num_features, num_classes)\n",
    "      input_cache: Input data used in the forward pass, shape (batch_size, num_features)\n",
    "                   or (num_features,) for a single example.\n",
    "    \n",
    "    Returns:\n",
    "      dinput: Gradient with respect to the input, shape (batch_size, num_features)\n",
    "      dweights: Gradient with respect to the weights, shape (num_features, num_classes)\n",
    "      dbiases: Gradient with respect to the biases, shape (num_classes,)\n",
    "    \"\"\"\n",
    "    # Ensure dout and input_cache are at least 2D.\n",
    "    dout = np.atleast_2d(dout)         # Now shape is (batch_size, num_classes)\n",
    "    input_cache = np.atleast_2d(input_cache)  # Now shape is (batch_size, num_features)\n",
    "\n",
    "    # Compute gradients\n",
    "    dinput = dout.dot(weights.T)       # (batch_size, num_classes) dot (num_classes, num_features) -> (batch_size, num_features)\n",
    "    dweights = input_cache.T.dot(dout)   # (num_features, batch_size) dot (batch_size, num_classes) -> (num_features, num_classes)\n",
    "    dbiases = np.sum(dout, axis=0)       # Sum over the batch dimension\n",
    "\n",
    "    # If you want to average gradients over the batch, you can uncomment the following lines:\n",
    "    # batch_size = input_cache.shape[0]\n",
    "    # dweights /= batch_size\n",
    "    # dbiases /= batch_size\n",
    "\n",
    "    # Optionally, if you require dinput to be 1D when the batch size is 1, you can do:\n",
    "    # if dinput.shape[0] == 1:\n",
    "    #     dinput = dinput.flatten()\n",
    "\n",
    "    return dinput, dweights, dbiases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(dout, caches, dropout_rate=0.5, training=True):\n",
    "    \"\"\"\n",
    "    Backward pass for the CNN with two convolutional layers and two fully connected layers.\n",
    "    \n",
    "    Parameters:\n",
    "      dout: Gradient from the loss w.r.t. the output of the softmax (shape: (batch_size, num_classes))\n",
    "      caches: A dictionary containing caches from the forward pass for:\n",
    "              - conv1_cache, conv2_cache: caches from the convolutional layers\n",
    "              - relu1_cache, relu2_cache: caches from the ReLU activations\n",
    "              - pool1_cache, pool2_cache: caches from the max pooling layers\n",
    "              - fc1_cache, fc2_cache: caches from the fully connected layers, which must include:\n",
    "                  - 'weights': the FC layer weights,\n",
    "                  - 'input': the FC layer input (flattened output from pooling),\n",
    "                  - 'output_shape': the shape of the pooled output before flattening.\n",
    "              - training (bool): Flag indicating whether dropout was applied.\n",
    "    Returns:\n",
    "      grads: Dictionary containing gradients for:\n",
    "             - 'fc1_w', 'fc1_b', 'fc2_w', 'fc2_b'\n",
    "             - 'conv1_w', 'conv1_b', 'conv2_w', 'conv2_b'\n",
    "    \"\"\"\n",
    "    # Unpack caches\n",
    "    conv1_cache = caches['conv1_cache']\n",
    "    relu1_cache = caches['relu1_cache']\n",
    "    pool1_cache = caches['pool1_cache']\n",
    "\n",
    "    conv2_cache = caches['conv2_cache']\n",
    "    relu2_cache = caches['relu2_cache']\n",
    "    pool2_cache = caches['pool2_cache']\n",
    "\n",
    "    fc1_cache = caches['fc1_cache']\n",
    "    fc2_cache = caches['fc2_cache']\n",
    "\n",
    "    grads = {}\n",
    "\n",
    "    # Backprop through Fully Connected Layer 2 (Output Layer)\n",
    "    dinput_fc1, grads['fc2_w'], grads['fc2_b'] = fc_backward(\n",
    "        dinput_fc2, fc2_cache['weights'], fc2_cache['input']\n",
    "    )\n",
    "\n",
    "    # Apply dropout backward for FC1 (if training)\n",
    "    if training and fc1_cache.get('dropout_cache') is not None:\n",
    "        dinput_fc1 = dropout_backward(dinput_fc1, fc1_cache['dropout_cache'])\n",
    "\n",
    "    # Backprop through Fully Connected Layer 1\n",
    "    dinput_pool, grads['fc1_w'], grads['fc1_b'] = fc_backward(\n",
    "        dinput_fc1, fc1_cache['weights'], fc1_cache['input']\n",
    "    )\n",
    "\n",
    "    # Reshape the gradient to the shape of the pooling layer output\n",
    "    dinput_pool = dinput_pool.reshape(fc1_cache['output_shape'])\n",
    "\n",
    "    # Backprop through Pooling Layer 2\n",
    "    dinput_relu2 = pool_backward(dinput_pool, pool2_cache)\n",
    "\n",
    "    # Backprop through ReLU 2\n",
    "    dinput_conv2 = relu_backward(dinput_relu2, relu2_cache)\n",
    "\n",
    "    # Backprop through Convolution Layer 2\n",
    "    dinput_pool1, grads['conv2_w'], grads['conv2_b'] = conv_backward(dinput_conv2, conv2_cache)\n",
    "\n",
    "    # Backprop through Pooling Layer 1\n",
    "    dinput_relu1 = pool_backward(dinput_pool1, pool1_cache)\n",
    "\n",
    "    # Backprop through ReLU 1\n",
    "    dinput_conv1 = relu_backward(dinput_relu1, relu1_cache)\n",
    "\n",
    "    # Backprop through Convolution Layer 1\n",
    "    _, grads['conv1_w'], grads['conv1_b'] = conv_backward(dinput_conv1, conv1_cache)\n",
    "    # print(grads['conv2_w'][0])\n",
    "    return grads\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(params, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update the parameters of the network using gradient descent with L2 regularization\n",
    "    and potentially adaptive learning rate techniques.\n",
    "\n",
    "    Args:\n",
    "    params (dict): Dictionary containing the parameters of the model.\n",
    "    grads (dict): Dictionary containing the gradients of the parameters.\n",
    "    learning_rate (float): Learning rate for the update step.\n",
    "\n",
    "    Returns:\n",
    "    dict: Updated parameters.\n",
    "    \"\"\"\n",
    "    # Regularization strength\n",
    "    reg_lambda = params.get('reg_lambda', 0.01)  # Default to some small lambda if not specified\n",
    "\n",
    "    # Update convolutional weights with L2 regularization\n",
    "    params['conv1_w'] -= learning_rate * (grads['conv1_w'] + reg_lambda * params['conv1_w'])\n",
    "    # print(grads['conv1_w'][0])\n",
    "    # params['conv1_w'] -= learning_rate * (grads['conv1_w'])\n",
    "    params['conv2_w'] -= learning_rate * (grads['conv2_w'] + reg_lambda * params['conv2_w'])\n",
    "    # params['conv2_w'] -= learning_rate * (grads['conv2_w'])\n",
    "\n",
    "    # Update fully connected layer weights with L2 regularization\n",
    "    params['fc1_w'] -= learning_rate * (grads['fc1_w'] + reg_lambda * params['fc1_w'])\n",
    "    params['fc2_w'] -= learning_rate * (grads['fc2_w'] + reg_lambda * params['fc2_w'])\n",
    "    # params['fc3_w'] -= learning_rate * (grads['fc3_w'] + reg_lambda * params['fc3_w'])\n",
    "    # params['fc1_w'] -= learning_rate * (grads['fc1_w'])\n",
    "    # params['fc2_w'] -= learning_rate * (grads['fc2_w'])\n",
    "    # params['fc3_w'] -= learning_rate * (grads['fc3_w'])\n",
    "    \n",
    "    # If biases are included in the params and grads, update them as well\n",
    "        # Biases typically do not have regularization applied\n",
    "    params['fc1_b'] -= learning_rate * grads['fc1_b']\n",
    "    params['fc2_b'] -= learning_rate * grads['fc2_b']\n",
    "    # params['fc3_b'] -= learning_rate * grads['fc3_b']\n",
    "    \n",
    "    # Update convolutional biases if provided (biases typically are not regularized).\n",
    "    params['conv1_b'] -= learning_rate * grads['conv1_b']\n",
    "    params['conv2_b'] -= learning_rate * grads['conv2_b']\n",
    "\n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def evaluate(X_val, y_val, params):\n",
    "    \"\"\"\n",
    "    Evaluate the model's accuracy on the validation set using batch processing.\n",
    "\n",
    "    X_val: Validation features (numpy array of shape [batch_size, features])\n",
    "    y_val: Validation labels (numpy array of shape [batch_size])\n",
    "    params: Trained parameters of the network\n",
    "    \n",
    "    Returns:\n",
    "    float: The accuracy of the model on the validation set\n",
    "    \"\"\"\n",
    "    # Process the entire validation set as a single batch\n",
    "    logits, _ = forward_pass(X_val, params, dropout_rate=0.5, training=True)  # forward_pass must handle batch processing\n",
    "    predicted_classes = np.argmax(logits, axis=1)  # axis=1 for batch processing\n",
    "\n",
    "    # Calculate the number of correctly predicted examples\n",
    "    total_correct = np.sum(predicted_classes == y_val)\n",
    "    accuracy = total_correct / len(X_val)\n",
    "\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (32,32) (32,27) (32,32) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 106\u001b[0m\n\u001b[0;32m    104\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.005\u001b[39m\n\u001b[0;32m    105\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[1;32m--> 106\u001b[0m trained_params \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[35], line 24\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(X_train, y_train, X_val, y_val, epochs, learning_rate, params, batch_size, dropout_rate)\u001b[0m\n\u001b[0;32m     21\u001b[0m y_batch \u001b[38;5;241m=\u001b[39m y_train[start:end]\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Forward pass with dropout (training=True)\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m softmax_output, caches \u001b[38;5;241m=\u001b[39m \u001b[43mforward_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Debug: Check for non-finite values in softmax_output\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(np\u001b[38;5;241m.\u001b[39misfinite(softmax_output)):\n",
      "Cell \u001b[1;32mIn[21], line 42\u001b[0m, in \u001b[0;36mforward_pass\u001b[1;34m(X_batch, params, dropout_rate, training)\u001b[0m\n\u001b[0;32m     39\u001b[0m caches \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# First Convolutional Layer\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m conv1_output, conv1_cache \u001b[38;5;241m=\u001b[39m \u001b[43mconvolve2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconv1_w\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconv1_b\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m bn1_output \u001b[38;5;241m=\u001b[39m batch_norm(conv1_output) \u001b[38;5;28;01mif\u001b[39;00m training \u001b[38;5;28;01melse\u001b[39;00m conv1_output  \u001b[38;5;66;03m# Apply Batch Normalization only during training\u001b[39;00m\n\u001b[0;32m     44\u001b[0m relu1_output \u001b[38;5;241m=\u001b[39m relu(bn1_output)        \u001b[38;5;66;03m# Apply ReLU\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[34], line 65\u001b[0m, in \u001b[0;36mconvolve2d\u001b[1;34m(images, kernel, bias, stride, padding)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(input_depth):\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;66;03m# Pad the input (here padding=1 is assumed; e.g. for a 3x3 filter)\u001b[39;00m\n\u001b[0;32m     64\u001b[0m     padded_input \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mpad(images[n, d], ((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 65\u001b[0m     \u001b[43mconv_result\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcorrelate2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpadded_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m[\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# Add bias and apply ReLU activation.\u001b[39;00m\n\u001b[0;32m     67\u001b[0m output[n, f] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmaximum(conv_result \u001b[38;5;241m+\u001b[39m bias[f], \u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (32,32) (32,27) (32,32) "
     ]
    }
   ],
   "source": [
    "import time\n",
    "def train(X_train, y_train, X_val, y_val, epochs, learning_rate, params, batch_size=32, dropout_rate=0.2):\n",
    "    n_samples = len(X_train)\n",
    "    decay_rate = 0.95  # Decay rate per epoch\n",
    "    min_lr = 0.001\n",
    "    max_lr = 0.01\n",
    "    for epoch in range(epochs):\n",
    "        loss_total = 0\n",
    "        # Learning rate decay\n",
    "        # current_lr = learning_rate\n",
    "        # current_lr = learning_rate * (0.95 ** epoch)\n",
    "        # current_lr = max(0.005, min(learning_rate * (decay_rate ** epoch), 0.01))\n",
    "        current_lr = min(0.01, learning_rate * (1 + 0.05 * epoch))  # Slowly increases up to 0.01\n",
    "        current_lr = min_lr + 0.5 * (max_lr - min_lr) * (1 + np.cos(np.pi * epoch /epochs))\n",
    "\n",
    "        for start in range(0, n_samples, batch_size):\n",
    "            \n",
    "            start_time = time.time()  # Start timing\n",
    "            end = start + batch_size\n",
    "            X_batch = X_train[start:end]\n",
    "            y_batch = y_train[start:end]\n",
    "\n",
    "            # Forward pass with dropout (training=True)\n",
    "            softmax_output, caches = forward_pass(X_batch, params, dropout_rate, training=True)\n",
    "            \n",
    "            # Debug: Check for non-finite values in softmax_output\n",
    "            if not np.all(np.isfinite(softmax_output)):\n",
    "                print(\"Warning: non-finite values found in softmax output.\")\n",
    "            \n",
    "            # Compute loss (with epsilon for numerical stability)\n",
    "            loss = compute_loss(softmax_output, y_batch)\n",
    "            if np.isnan(loss) or np.isinf(loss):\n",
    "                print(\"Warning: loss is non-finite!\")\n",
    "            loss_total += loss\n",
    "\n",
    "            # Backward pass with dropout (training=True)\n",
    "            grads = backward_pass(softmax_output, caches, dropout_rate, training=True)\n",
    "            \n",
    "            # Optionally clip gradients to prevent exploding gradients\n",
    "            grads = clip_gradients(grads, max_norm=2.0)\n",
    "            \n",
    "            # Update parameters\n",
    "            params = update_params(params, grads, current_lr)\n",
    "            end_time = time.time()  # End timing\n",
    "\n",
    "            # Uncomment if you want to print batch details:\n",
    "            # print(f\"Batch from {start} to {end}, loss: {loss}, total: {n_samples}, Time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "        val_accuracy = evaluate(X_val, y_val, params)\n",
    "        print(f'Epoch {epoch + 1}/{epochs}, Average Loss: {loss_total / (n_samples // batch_size)}, Validation Accuracy: {val_accuracy}')\n",
    "    \n",
    "    return params\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "def init_params():\n",
    "    params = {}\n",
    "\n",
    "    # He Initialization for ReLU activations\n",
    "    def he_init(shape):\n",
    "        return np.random.randn(*shape) * np.sqrt(2.0 / np.prod(shape[:-1]))\n",
    "\n",
    "    # Convolutional Layer 1 Parameters\n",
    "    kernel_height1, kernel_width1, input_channels1, num_filters1 = 3, 3, 3, 8\n",
    "    params['conv1_w'] = he_init((kernel_height1, kernel_width1, input_channels1, num_filters1))\n",
    "    params['conv1_b'] = np.zeros((1, 1, 1, num_filters1))\n",
    "\n",
    "    # Convolutional Layer 2 Parameters\n",
    "    kernel_height2, kernel_width2, input_channels2, num_filters2 = 3, 3, num_filters1, 16\n",
    "    params['conv2_w'] = he_init((kernel_height2, kernel_width2, input_channels2, num_filters2))\n",
    "    params['conv2_b'] = np.zeros((1, 1, 1, num_filters2))\n",
    "\n",
    "    # Fully Connected Layer 1 Parameters\n",
    "    num_inputs_fc1, num_outputs_fc1 = 1024, 512\n",
    "    params['fc1_w'] = he_init((num_inputs_fc1, num_outputs_fc1))\n",
    "    params['fc1_b'] = np.zeros((1, num_outputs_fc1))\n",
    "\n",
    "    # Fully Connected Layer 2 (Output Layer) Parameters\n",
    "    num_inputs_fc2, num_outputs_fc2 = num_outputs_fc1, 10  # Assuming 10 output classes\n",
    "    params['fc2_w'] = he_init((num_inputs_fc2, num_outputs_fc2))\n",
    "    params['fc2_b'] = np.zeros((1, num_outputs_fc2))\n",
    "\n",
    "    # Regularization strength\n",
    "    params['reg_lambda'] = 0.001\n",
    "\n",
    "    return params\n",
    "\n",
    "\n",
    "\n",
    "def clip_gradients(grads, max_norm=2.0):\n",
    "    total_norm = np.sqrt(sum(np.sum(g ** 2) for g in grads.values()))\n",
    "    if total_norm > max_norm:\n",
    "        scaling_factor = max_norm / (total_norm + 1e-6)\n",
    "        for key in grads:\n",
    "            grads[key] *= scaling_factor\n",
    "    return grads\n",
    "\n",
    "\n",
    "\n",
    "params = init_params()\n",
    "num_epochs = 1000\n",
    "learning_rate = 0.005\n",
    "batch_size = 32\n",
    "trained_params = train(train_features, train_labels, validate_features, validate_labels, num_epochs, learning_rate, params,batch_size)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
