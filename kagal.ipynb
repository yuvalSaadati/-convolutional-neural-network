{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "from IPython.display import clear_output, display\n",
    "import matplotlib.pyplot as plt\n",
    "from time import sleep\n",
    "def load_data(filename):\n",
    "    \"\"\"\n",
    "    Loads data from a CSV file and processes it into suitable format.\n",
    "    For training and validation data, it splits into features and labels.\n",
    "    For test data, it returns only features and assumes the first column is a placeholder.\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(filename, header=None)\n",
    "    if 'train' in filename or 'validate' in filename:\n",
    "        # Split into features and labels\n",
    "        labels = data.iloc[:, 0].values - 1  # Convert class labels to zero-indexed\n",
    "\n",
    "        # labels = data.iloc[:, 0].values   # Convert class labels to zero-indexed\n",
    "        features = data.iloc[:, 1:].values\n",
    "    else:\n",
    "        # Test data does not include labels\n",
    "        labels = None\n",
    "        features = data.iloc[:, 1:].values  # Ignore the placeholder column\n",
    "    \n",
    "    # Reshape features into 32x32x3 format for RGB images\n",
    "    features = features.reshape((-1, 32, 32, 3))\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "# Example usage\n",
    "X_train_data, y_train_data = load_data('./data/train.csv')\n",
    "X_validate_data, y_validate_data = load_data('./data/validate.csv')\n",
    "X_test_data, _ = load_data('./data/test.csv')  # Test data doesn't have labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 215\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m501\u001b[39m):\n\u001b[1;32m--> 215\u001b[0m     grads \u001b[38;5;241m=\u001b[39m \u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mFILTER_SIZE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     grads \u001b[38;5;241m=\u001b[39m average_grads(grads)\n\u001b[0;32m    217\u001b[0m     grads, theta, convds, filters \u001b[38;5;241m=\u001b[39m optimize(grads, theta, convds, filters, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[12], line 79\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(x, y, theta, convds, filters, f)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward\u001b[39m(x, y, theta, convds, filters, f):\n\u001b[0;32m     78\u001b[0m     l, l1, w1, w2, w3, b1, b2, b3 \u001b[38;5;241m=\u001b[39m theta\n\u001b[1;32m---> 79\u001b[0m     m, n, o, flat, r, probs \u001b[38;5;241m=\u001b[39m \u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m     dout \u001b[38;5;241m=\u001b[39m probs \u001b[38;5;241m-\u001b[39m y\u001b[38;5;241m.\u001b[39mreshape(y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m     83\u001b[0m     dw3 \u001b[38;5;241m=\u001b[39m flat\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mdot(dout)\n",
      "Cell \u001b[1;32mIn[12], line 51\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(x, theta, convds, filters, f)\u001b[0m\n\u001b[0;32m     48\u001b[0m m \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([conv(l1, w1, b1, f, c1, f1, x[i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(x))])\n\u001b[0;32m     49\u001b[0m m \u001b[38;5;241m=\u001b[39m relu(m)\n\u001b[1;32m---> 51\u001b[0m n \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43ml2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(m))])\n\u001b[0;32m     52\u001b[0m n \u001b[38;5;241m=\u001b[39m relu(n)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# size 2, stride 2\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[12], line 23\u001b[0m, in \u001b[0;36mconv\u001b[1;34m(l, w, b, f, convd, filter, image)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, w):\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, w):\n\u001b[1;32m---> 23\u001b[0m             convd[jj,i,j] \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43mj\u001b[49m\u001b[43m:\u001b[49m\u001b[43mj\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mjj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m b[jj]\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m convd\n",
      "File \u001b[1;32mc:\\Users\\Yuval\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:2313\u001b[0m, in \u001b[0;36msum\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2310\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m   2311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[1;32m-> 2313\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2314\u001b[0m \u001b[43m                      \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Yuval\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:71\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m         \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     62\u001b[0m         \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     67\u001b[0m         \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m---> 71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrapreduction\u001b[39m(obj, ufunc, method, axis, dtype, out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     72\u001b[0m     passkwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m     73\u001b[0m                   \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39m_NoValue}\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(obj) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mu\u001b[38;5;241m.\u001b[39mndarray:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# (X_train, y_train) , (X_test, y_test) = cifar10.load_data()\n",
    "print(len(X_train_data))\n",
    "X_train = X_train_data.reshape(len(X_train_data), 3, 32, 32) / 255.0\n",
    "X_validate = X_validate_data.reshape(len(X_validate_data), 3, 32, 32) / 255.0\n",
    "X_test = X_test_data.reshape(len(X_test_data), 3, 32, 32) / 255.0\n",
    "y_train = np.eye(10)[y_train_data]\n",
    "y_validate = np.eye(10)[y_validate_data]\n",
    "\n",
    "def initialise_param_lecun_normal(FILTER_SIZE, IMG_DEPTH, scale=1.0):\n",
    "    fan_in = FILTER_SIZE * FILTER_SIZE * IMG_DEPTH\n",
    "    stddev = scale * np.sqrt(1./fan_in)\n",
    "    shape = (IMG_DEPTH, FILTER_SIZE, FILTER_SIZE)\n",
    "    return np.random.normal(loc = 0,scale = stddev,size = shape) / 9\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "def relu(x):\n",
    "    x[x < 0] = 0\n",
    "    return x\n",
    "def conv(l, w, b, f, convd, filter, image):\n",
    "    for jj in range(0, l):\n",
    "        for i in range(0, w):\n",
    "            for j in range(0, w):\n",
    "                convd[jj,i,j] = np.sum(image[:,i:i+f,j:j+f] * filter[jj]) + b[jj]\n",
    "\n",
    "    return convd\n",
    "def maxpool(x, f, s):\n",
    "    (l, w, w) = x.shape\n",
    "    pool = np.zeros((l, (w-f)//s+1,(w-f)//s+1))\n",
    "    for jj in range(0,l):\n",
    "        for i in range(0, w, s):\n",
    "            for j in range(0, w, s):\n",
    "                pool[jj,i//2,j//2] = np.max(x[jj,i:i+f,j:j+f])\n",
    "    return pool\n",
    "def nanargmax(a):\n",
    "\tidx = np.argmax(a, axis=None)\n",
    "\tmulti_idx = np.unravel_index(idx, a.shape)\n",
    "\tif np.isnan(a[multi_idx]):\n",
    "\t\tnan_count = np.sum(np.isnan(a))\n",
    "\t\tidx = np.argpartition(a, -nan_count-1, axis=None)[-nan_count-1]\n",
    "\t\tmulti_idx = np.unravel_index(idx, a.shape)\n",
    "\treturn multi_idx\n",
    "\n",
    "def forward(x, theta, convds, filters, f):\n",
    "    l, l1, w1, w2, w3, b1, b2, b3 = theta\n",
    "    f1, f2 = filters\n",
    "    c1, c2 = convds\n",
    "\n",
    "    m = np.array([conv(l1, w1, b1, f, c1, f1, x[i]) for i in range(len(x))])\n",
    "    m = relu(m)\n",
    "\n",
    "    n = np.array([conv(l2, w2, b2, f, c2, f2, m[i]) for i in range(len(m))])\n",
    "    n = relu(n)\n",
    "\n",
    "    # size 2, stride 2\n",
    "    o = np.array([maxpool(n[i], 2, 2) for i in range(len(n))])\n",
    "\n",
    "    # flatten\n",
    "    flat = o.reshape((len(o), (w2//2) * (w2//2) *l2))\n",
    "\n",
    "    r = flat.dot(w3) + b3\n",
    "\n",
    "    probs = np.array([softmax(r[i]) for i in range(len(r))])\n",
    "\n",
    "    return m, n, o, flat, r, probs\n",
    "def dfilter_init(n, l2, l1, f):\n",
    "    df, df_sub = [], []\n",
    "    db, db_sub = [], []\n",
    "    for _ in range(0, n):\n",
    "        for x in range(0, l2):\n",
    "            df_sub.append(np.zeros((l1,f,f)))\n",
    "            db_sub.append(0)\n",
    "        df.append(df_sub)\n",
    "        db.append(db_sub)\n",
    "        df_sub, db_sub = [], []\n",
    "    \n",
    "    return np.array(df), np.array(db)\n",
    "def backward(x, y, theta, convds, filters, f):\n",
    "    l, l1, w1, w2, w3, b1, b2, b3 = theta\n",
    "    m, n, o, flat, r, probs = forward(x, theta, convds, filters, f)\n",
    "\n",
    "    dout = probs - y.reshape(y.shape[0], 10)\n",
    "\n",
    "    dw3 = flat.T.dot(dout)\n",
    "    db3 = np.expand_dims(np.sum(dout, axis=0), axis=0)\n",
    "\n",
    "    df = dout.dot(w3.T)\n",
    "\n",
    "    dpool = df.T.reshape((x.shape[0], l2, w2//2, w2//2))\n",
    "    dc2 = np.zeros((len(n), l2, w2, w2))\n",
    "\n",
    "    for nn in range(len(n)):\n",
    "        for jj in range(0,l):\n",
    "            for i in range(0, w2, 2):\n",
    "                for j in range(0, w2, 2):\n",
    "                    (a,b) = nanargmax(n[nn][jj,i:i+2,j:j+2])\n",
    "                    dc2[nn][jj, i+a, j+b] = dpool[nn][jj, i//2, j//2]\n",
    "\n",
    "    dc2[n <= 0] = 0\n",
    "\n",
    "    dc1 = np.zeros((len(m), l1, w1, w1))\n",
    "\n",
    "    df2, db2 = dfilter_init(len(m), l2, l1, f)\n",
    "    df1, db1 = dfilter_init(len(m), l1, l, f)\n",
    "\n",
    "    for mm in range(len(m)):\n",
    "        for jj in range(0, l2):\n",
    "            for i in range(0, w2):\n",
    "                for j in range(0, w2):\n",
    "                    df2[mm][jj] += dc2[mm][jj, i, j] * m[mm][:, i:i+f, j:j+f]\n",
    "                    dc1[mm][:, i:i+f, j:j+f] += dc2[mm][jj, i, j] * f2[jj]\n",
    "            db2[mm][jj] = np.sum(dc2[mm][jj])\n",
    "\n",
    "    dc1[m <= 0]=0\n",
    "\n",
    "    for mm in range(len(m)):\n",
    "        for jj in range(0, l1):\n",
    "            for i in range(0, w1):\n",
    "                for j in range(0, w1):\n",
    "                    df1[mm][jj] += dc1[mm][jj, i, j] * x[mm][:, i:i+f, j:j+f]\n",
    "            db1[mm][jj] = np.sum(dc1[mm][jj])\n",
    "\n",
    "    return dc1, dc2, df1, df2, dw3, db1, db2, db3\n",
    "def average_grads(grads):\n",
    "    return [np.average(grads[i], axis=0) for i in range(len(grads))]\n",
    "def optimize(grads, theta, convds, filters, lr=0.01):\n",
    "    dc1, dc2, df1, df2, dw3, db1, db2, db3 = grads\n",
    "    l, l1, w1, w2, w3, b1, b2, b3 = theta\n",
    "    c1, c2 = convds\n",
    "    f1, f2 = filters\n",
    "\n",
    "    c1 -= dc1 * lr\n",
    "    c2 -= dc2 * lr\n",
    "\n",
    "    f1 -= df1 * lr\n",
    "    f2 -= df2 * lr\n",
    "\n",
    "    w3 -= dw3 * lr\n",
    "\n",
    "    b1 -= db1 * lr\n",
    "    b2 -= db2 * lr\n",
    "    b3 -= db3 * lr\n",
    "\n",
    "    grads = dc1, dc2, df1, df2, dw3, db1, db2, db3\n",
    "    theta = l, l1, w1, w2, w3, b1, b2, b3\n",
    "    convds = c1, c2\n",
    "    filters = f1, f2\n",
    "\n",
    "    return grads, theta, convds, filters\n",
    "def cross_entropy(predictions, targets, epsilon=1e-12):\n",
    "    \"\"\"\n",
    "    Computes cross entropy between targets (encoded as one-hot vectors)\n",
    "    and predictions. \n",
    "    Input: predictions (N, k) ndarray\n",
    "           targets (N, k) ndarray        \n",
    "    Returns: scalar\n",
    "    \"\"\"\n",
    "    predictions = np.clip(predictions, epsilon, 1. - epsilon)\n",
    "    N = predictions.shape[0]\n",
    "    ce = -np.sum(targets*np.log(predictions+1e-9))/N\n",
    "    return ce\n",
    "# because CPU convolutional takes forever to train\n",
    "# we select : 5 idx where the number is 9 and 5 idx for number 1\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming X_train, y_train, X_validate, y_validate, X_test are already defined and preprocessed correctly\n",
    "\n",
    "np.random.seed(2342342)  # For reproducibility\n",
    "\n",
    "# Hyperparameters and model specifications\n",
    "NUM_FILT1 = 16\n",
    "NUM_FILT2 = 16\n",
    "IMG_DEPTH = 3\n",
    "FILTER_SIZE = 5\n",
    "\n",
    "# Initialize parameters\n",
    "def initialise_param_lecun_normal(FILTER_SIZE, IMG_DEPTH, scale=1.0):\n",
    "    fan_in = FILTER_SIZE * FILTER_SIZE * IMG_DEPTH\n",
    "    stddev = scale * np.sqrt(1./fan_in)\n",
    "    shape = (IMG_DEPTH, FILTER_SIZE, FILTER_SIZE)\n",
    "    return np.random.normal(loc=0, scale=stddev, size=shape) / 9\n",
    "\n",
    "f1 = [initialise_param_lecun_normal(FILTER_SIZE, IMG_DEPTH) for _ in range(NUM_FILT1)]\n",
    "b1 = [0. for _ in range(NUM_FILT1)]\n",
    "f2 = [initialise_param_lecun_normal(FILTER_SIZE, NUM_FILT1) for _ in range(NUM_FILT2)]\n",
    "b2 = [0. for _ in range(NUM_FILT2)]\n",
    "w3 = np.random.normal(size=(2304, 10)) / 9\n",
    "b3 = np.random.normal(size=(1, 10)) / 9\n",
    "\n",
    "# Dimensions based on filter sizes and input\n",
    "l, w, _ = X_train[0].shape  # assuming X_train[0] has the shape of (depth, width, height)\n",
    "l1, l2 = len(f1), len(f2)\n",
    "w1 = w - FILTER_SIZE + 1\n",
    "w2 = w1 - FILTER_SIZE + 1\n",
    "\n",
    "# Setup for convolutional and fully connected layers\n",
    "c1 = np.zeros((l1, w1, w1))\n",
    "c2 = np.zeros((l2, w2, w2))\n",
    "theta = l, l1, w1, w2, w3, b1, b2, b3\n",
    "convds = c1, c2\n",
    "filters = np.array(f1), np.array(f2)\n",
    "\n",
    "losses, accuracies = [], []\n",
    "\n",
    "# Function for calculating accuracy\n",
    "def calculate_accuracy(predictions, labels):\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = np.argmax(labels, axis=1)\n",
    "    correct_predictions = np.sum(predicted_classes == true_classes)\n",
    "    accuracy = correct_predictions / len(labels) * 100\n",
    "    return accuracy\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(501):\n",
    "    grads = backward(X_train, y_train, theta, convds, filters, FILTER_SIZE)\n",
    "    grads = average_grads(grads)\n",
    "    grads, theta, convds, filters = optimize(grads, theta, convds, filters, lr=0.1)\n",
    "\n",
    "    #if epoch % 25 == 0:\n",
    "    train_out = forward(X_train, theta, convds, filters, FILTER_SIZE)[-1]\n",
    "    train_loss = cross_entropy(train_out, y_train)\n",
    "    train_accuracy = calculate_accuracy(train_out, y_train)\n",
    "\n",
    "    validate_out = forward(X_validate, theta, convds, filters, FILTER_SIZE)[-1]\n",
    "    validate_loss = cross_entropy(validate_out, y_validate)\n",
    "    validate_accuracy = calculate_accuracy(validate_out, y_validate)\n",
    "\n",
    "    losses.append(train_loss)\n",
    "    accuracies.append(validate_accuracy)\n",
    "\n",
    "    print(f'Epoch: {epoch:4d}, Train Loss: {train_loss:.3f}, Train Accuracy: {train_accuracy:.2f}%, Validate Loss: {validate_loss:.3f}, Validate Accuracy: {validate_accuracy:.2f}%')\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.plot(losses)\n",
    "plt.title('Loss over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "# After training, predict on test data\n",
    "test_out = forward(X_test, theta, convds, filters, FILTER_SIZE)[-1]\n",
    "test_predictions = np.argmax(test_out, axis=1)\n",
    "\n",
    "# Assuming you have a way to evaluate or utilize test predictions\n",
    "# Here you might save or process the predictions further\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
